
NOTES:

# Categorical are prefered for Decison Tree

# Prefered for Analysis rather than for predictive Analysis

# Similar to KNN( where NO training set is available so has overhead of testing though is NOT Unsupervised)

# Works By Splitting the attributes Parallel to axis

# The Attribute with the maximum Information is to be Splitted first

# Information gain:
    - Entropy of system before Split - Entropy of System after Split
    (sum of individual Entropie = system entropy)

# Decision tree breaks/splits the tree on basis of the Information gain( on which attribute to split the tree on)
   - Entropy(https://www.youtube.com/watch?v=AmCV4g7_-QM)
   - gini Index
   
   
#  Regression + classification(CART):
  - Binary Split(Only Binary Split)
  - Gini Index
  
# C5.0
  - MultiSplit
  - Infogain(Entropy)
  
  
# Resolutiion for OverFitting:
  - Pruning Tree
          - Cost Complexity Pruning(in CART)
            - Higher the CART Value : less is Size of Tree
          - Pessimistic Pruning(in C5.0)

# MultiColiniearity NOT a concern in Decison Tree case
  


# Overfitting and Pruning


# Ensembles:
    To Club multiple Weak Models to make ONE Strong Build,
    hence BAGGING

install.packages("ctree")


str(iris)

iris_ctree <- c(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data=iris)

print(iris_ctree)

plot(iris_ctree)



library("rpart")

data <- read.csv("C:\\Users\\hp\\Downloads\\new_R_exercises\\universalBank.csv")

str(data)
View(data)

tree <- rpart(Experience ~ adm_data$Age + adm_data$Mortgage+ adm_data$Income + adm_data$Family, data=adm_data, method="class")


plot(tree)

text(tree, pretty=0)




