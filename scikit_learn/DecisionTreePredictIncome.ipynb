{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref: https://www.kaggle.com/uciml/adult-census-income\n",
    "\n",
    "soln:\n",
    "  1. https://www.kaggle.com/kyleockerlund/census-data-prediction\n",
    "  \n",
    "  Questions:\n",
    "  1). Is Cross Validation Vs predict to find ML estimate????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # read and wrangle dataframes\n",
    "import matplotlib.pyplot as plt # visualization\n",
    "import seaborn as sns # statistical visualizations and aesthetics\n",
    "from sklearn.base import TransformerMixin # To create new classes for transformations\n",
    "from sklearn.preprocessing import (FunctionTransformer, StandardScaler) # preprocessing \n",
    "from sklearn.decomposition import PCA # dimensionality reduction\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from scipy.stats import boxcox # data transform\n",
    "from sklearn.model_selection import (train_test_split, KFold , StratifiedKFold, \n",
    "                                     cross_val_score, GridSearchCV, \n",
    "                                     learning_curve, validation_curve) # model selection modules\n",
    "from sklearn.pipeline import Pipeline # streaming pipelines\n",
    "from sklearn.base import BaseEstimator, TransformerMixin # To create a box-cox transformation class\n",
    "from collections import Counter\n",
    "import warnings\n",
    "# load models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#from xgboost import (XGBClassifier, plot_importance)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from time import time\n",
    "\n",
    "%matplotlib inline \n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch the data set as .csv file from local disk space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('adult.csv')\n",
    "#features = df.columns.tolist()\n",
    "#print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.columns[:-1].tolist()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the count of observation with the DataType for the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals = pd.DataFrame({\"gains\":df['capital.gain'], \"losses\":df['capital.loss']})\n",
    "capitals.hist(bins = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['income'] = df['income'].replace({'<=50K': 0, '>50K':1}, regex=True)\n",
    "\n",
    "#df = df.convert_objects(convert_numeric=True)\n",
    "print(df.dtypes)\n",
    "#pd.to_numeric(features, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['income'].hist(bins = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After Imputations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = df.columns[:-1].tolist()\n",
    "\n",
    "for feat in features:\n",
    "    skew = df[feat].skew()\n",
    "    sns.distplot(df[feat], kde= False, label='Skew = %.3f' %(skew), bins=30)\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define X as features and y as lablels\n",
    "X = df[features] \n",
    "y = df['income'] \n",
    "# set a seed and a test size for splitting the dataset \n",
    "seed = 7\n",
    "test_size = 0.2\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size , random_state = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding outlier seems time consuming operation: WHY???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "    Takes a dataframe df of features and returns a list of the indices\n",
    "    corresponding to the observations containing more than 2 outliers. \n",
    "\n",
    "  outlier_indices = []\n",
    "    \n",
    "    # iterate over features(columns)\n",
    "    for col in df.columns.tolist():\n",
    "        # 1st quartile (25%)\n",
    "        Q1 = np.percentile(df[col], 25)\n",
    "        \n",
    "        # 3rd quartile (75%)\n",
    "        Q3 = np.percentile(df[col],75)\n",
    "        \n",
    "        # Interquartile rrange (IQR)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # outlier step\n",
    "        outlier_step = 1.5 * IQR\n",
    "        \n",
    "        # Determine a list of indices of outliers for feature col\n",
    "        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n",
    "        \n",
    "        # append the found outlier indices for col to the list of outlier indices \n",
    "        outlier_indices.extend(outlier_list_col)\n",
    "        \n",
    "    # select observations containing more than 2 outliers\n",
    "    outlier_indices = Counter(outlier_indices)        \n",
    "    multiple_outliers = list( k for k, v in outlier_indices.items() if v > 2 )\n",
    "    \n",
    "    return multiple_outliers   \n",
    "\n",
    "print('The dataset contains %d observations with more than 2 outliers' %(len(outlier_hunt(df[features]))))\n",
    "\"\"\"\n",
    "\n",
    "# Detect observations with more than one outlier\n",
    "\n",
    "# def outlier_hunt(df):\n",
    "\n",
    "\t\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"plt.figure(figsize=(8,8))\n",
    "sns.pairplot(df[features],palette='coolwarm')\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['income'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the Corelation between the pairs of input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''corr = df[features].corr()\n",
    "plt.figure(figsize=(28,28))\n",
    "sns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 15},\n",
    "           xticklabels= features, yticklabels= features, alpha = 0.7,   cmap= 'coolwarm')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = df.drop('income', axis = 1)\n",
    "target = df.income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn = neighbors.KNeighborsClassifier(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = cross_val_score(knn, data, target, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "scores = []\n",
    "for k in range(1, 10):\n",
    "    knn = neighbors.KNeighborsClassifier(k)\n",
    "    scores.append(cross_val_score(knn, data, target, cv=2).mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.Series(scores)\n",
    "scores.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###residual error plot?????????????????????????????????????????????????????????????????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the commulative variaance to determine the most relevant features??? is PCA a commonly used technique for dimensionalityy reduction????? VS\n",
    "How to use feature_importances_ API against the Dimensional Reduction using PCA????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(random_state = seed)\n",
    "pca.fit(X_train)\n",
    "var_exp = pca.explained_variance_ratio_\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.bar(range(1,len(cum_var_exp)+1), var_exp, align= 'center', label= 'individual variance explained', \\\n",
    "       alpha = 0.7)\n",
    "plt.step(range(1,len(cum_var_exp)+1), cum_var_exp, where = 'mid' , label= 'cumulative variance explained', \\\n",
    "        color= 'red')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal components')\n",
    "plt.xticks(np.arange(1,len(var_exp)+1,1))\n",
    "plt.legend(loc='center right')\n",
    "plt.show()\n",
    "\n",
    "# Cumulative variance explained\n",
    "for i, sum in enumerate(cum_var_exp):\n",
    "    print(\"PC\" + str(i+1), \"Cumulative variance: %.3f% %\" %(cum_var_exp[i]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 5\n",
    "pipelines = []\n",
    "n_estimators = 200\n",
    "\n",
    "#print(df.shape)\n",
    "pipelines.append( ('SVC',\n",
    "                   Pipeline([\n",
    "                              ('sc', StandardScaler()),\n",
    "#                               ('pca', PCA(n_components = n_components, random_state=seed ) ),\n",
    "                             ('SVC', SVC(random_state=seed))]) ) )\n",
    "\n",
    "\n",
    "pipelines.append(('KNN',\n",
    "                  Pipeline([ \n",
    "                              ('sc', StandardScaler()),\n",
    "#                             ('pca', PCA(n_components = n_components, random_state=seed ) ),\n",
    "                            ('KNN', KNeighborsClassifier()) ])))\n",
    "pipelines.append( ('RF',\n",
    "                   Pipeline([\n",
    "                              ('sc', StandardScaler()),\n",
    "#                              ('pca', PCA(n_components = n_components, random_state=seed ) ), \n",
    "                             ('RF', RandomForestClassifier(random_state=seed, n_estimators=n_estimators)) ]) ))\n",
    "\n",
    "\n",
    "pipelines.append( ('Ada',\n",
    "                   Pipeline([ \n",
    "                              ('sc', StandardScaler()),\n",
    "#                              ('pca', PCA(n_components = n_components, random_state=seed ) ), \n",
    "                    ('Ada', AdaBoostClassifier(random_state=seed,  n_estimators=n_estimators)) ]) ))\n",
    "\n",
    "pipelines.append( ('ET',\n",
    "                   Pipeline([\n",
    "                              ('sc', StandardScaler()),\n",
    "#                              ('pca', PCA(n_components = n_components, random_state=seed ) ), \n",
    "                             ('ET', ExtraTreesClassifier(random_state=seed, n_estimators=n_estimators)) ]) ))\n",
    "pipelines.append( ('GB',\n",
    "                   Pipeline([ \n",
    "                             ('sc', StandardScaler()),\n",
    "#                             ('pca', PCA(n_components = n_components, random_state=seed ) ), \n",
    "                             ('GB', GradientBoostingClassifier(random_state=seed)) ]) ))\n",
    "\n",
    "pipelines.append( ('LR',\n",
    "                   Pipeline([\n",
    "                              ('sc', StandardScaler()),\n",
    "#                               ('pca', PCA(n_components = n_components, random_state=seed ) ), \n",
    "                             ('LR', LogisticRegression(random_state=seed)) ]) ))\n",
    "\n",
    "results, names, times  = [], [] , []\n",
    "num_folds = 10\n",
    "scoring = 'accuracy'\n",
    "\n",
    "for name, model in pipelines:\n",
    "    start = time()\n",
    "    kfold = StratifiedKFold(n_splits=num_folds, random_state=seed)\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring = scoring,\n",
    "                                n_jobs=-1) \n",
    "    t_elapsed = time() - start\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    times.append(t_elapsed)\n",
    "    msg = \"%s: %f (+/- %f) performed in %f seconds\" % (name, 100*cv_results.mean(), \n",
    "                                                       100*cv_results.std(), t_elapsed)\n",
    "    print(msg)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))    \n",
    "fig.suptitle(\"Algorithms comparison\")\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
